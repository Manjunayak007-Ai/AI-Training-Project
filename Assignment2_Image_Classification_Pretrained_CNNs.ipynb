### Notes on hyperparameters

- Optimizer: default AdamW with weight decay 1e-4. Try SGD (momentum 0.9) for comparison.
- Schedulers: cosine, step(7, 0.1), or one-cycle. Adjust `epochs` and `base_lr`.
- Augmentations: RandomResizedCrop, HorizontalFlip, mild ColorJitter. Avoid heavy aug on small datasets.
- Batch size: 64 by default; reduce if OOM.
- Freezing: You may set `freeze_backbone=True` for quick linear-probe, then unfreeze to fine-tune.
- Reproducibility: seeds fixed, CuDNN deterministic; slight variability may remain on GPU.# Run: train models and evaluate on test
image_datasets, dataloaders, class_names, dataset_sizes = create_datasets_and_loaders(cfg, split_dirs)
device = get_device()

results = {}
trained_models = {}

for model_name in ["resnet18", "resnet34", "vgg19"]:
    print("\n=== Training", model_name, "===")
    model = build_model(model_name, num_classes=len(class_names), pretrained=True, freeze_backbone=False)
    model, history, info = train_model(cfg, model, dataloaders, dataset_sizes, device, model_name)
    plot_history(history, title_prefix=model_name)

    # Save final explicitly per requirements
    final_path = os.path.join(cfg.save_dir, f"{model_name}.pth")
    torch.save(model.state_dict(), final_path)

    # Evaluate on test
    criterion = nn.CrossEntropyLoss()
    test_loss, test_acc = evaluate(model, dataloaders["test"], criterion, device)

    results[model_name] = {"best_val_acc": info["best_val_acc"], "test_acc": test_acc}
    trained_models[model_name] = model
    print(f"Test loss={test_loss:.4f} acc={test_acc:.4f}")

print("\nResults:")
for k, v in results.items():
    print(k, v)# Plotting utilities

def plot_history(history, title_prefix: str = ""):
    epochs = range(1, len(history["train_loss"]) + 1)
    fig, axs = plt.subplots(1, 2, figsize=(12, 4))

    axs[0].plot(epochs, history["train_loss"], label="train")
    axs[0].plot(epochs, history["val_loss"], label="val")
    axs[0].set_title(f"{title_prefix} Loss")
    axs[0].set_xlabel("Epoch")
    axs[0].set_ylabel("Loss")
    axs[0].legend()

    axs[1].plot(epochs, history["train_acc"], label="train")
    axs[1].plot(epochs, history["val_acc"], label="val")
    axs[1].set_title(f"{title_prefix} Accuracy")
    axs[1].set_xlabel("Epoch")
    axs[1].set_ylabel("Accuracy")
    axs[1].legend()

    plt.show()# Training and evaluation loops

def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    running_corrects = 0
    total = 0

    for inputs, labels in dataloader:
        inputs = inputs.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels).item()
        total += inputs.size(0)

    epoch_loss = running_loss / total
    epoch_acc = running_corrects / total
    return epoch_loss, epoch_acc


def evaluate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels).item()
            total += inputs.size(0)

    epoch_loss = running_loss / total
    epoch_acc = running_corrects / total
    return epoch_loss, epoch_acc


def train_model(cfg: Config, model: nn.Module, dataloaders, dataset_sizes, device, model_name: str, save_best=True):
    criterion = nn.CrossEntropyLoss()
    optimizer = build_optimizer(cfg, model)
    scheduler = build_scheduler(cfg, optimizer)

    model = model.to(device)
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    history = {"train_loss": [], "train_acc": [], "val_loss": [], "val_acc": []}

    start_time = time.time()
    for epoch in range(cfg.epochs):
        t0 = time.time()
        train_loss, train_acc = train_one_epoch(model, dataloaders["train"], criterion, optimizer, device)
        val_loss, val_acc = evaluate(model, dataloaders["val"], criterion, device)

        if scheduler is not None and not isinstance(scheduler, OneCycleLR):
            scheduler.step()

        history["train_loss"].append(train_loss)
        history["train_acc"].append(train_acc)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)

        if val_acc > best_acc:
            best_acc = val_acc
            best_model_wts = copy.deepcopy(model.state_dict())
            if save_best:
                os.makedirs(cfg.save_dir, exist_ok=True)
                best_path = os.path.join(cfg.save_dir, f"{model_name}_best.pth")
                torch.save(best_model_wts, best_path)

        print(f"Epoch {epoch+1}/{cfg.epochs} - {format_time(time.time()-t0)} | "
              f"train_loss={train_loss:.4f} acc={train_acc:.4f} | "
              f"val_loss={val_loss:.4f} acc={val_acc:.4f}")

    elapsed = format_time(time.time() - start_time)
    print("Training completed in", elapsed, "; Best val acc:", best_acc)

    model.load_state_dict(best_model_wts)
    final_path = os.path.join(cfg.save_dir, f"{model_name}_final.pth")
    torch.save(model.state_dict(), final_path)

    return model, history, {"best_val_acc": best_acc, "elapsed": elapsed}# Optimizer and scheduler

def build_optimizer(cfg: Config, model: nn.Module):
    params = [p for p in model.parameters() if p.requires_grad]
    if cfg.optimizer == "sgd":
        return optim.SGD(params, lr=cfg.base_lr, momentum=0.9, weight_decay=cfg.weight_decay, nesterov=True)
    elif cfg.optimizer == "adam":
        return optim.Adam(params, lr=cfg.base_lr, weight_decay=cfg.weight_decay)
    elif cfg.optimizer == "adamw":
        return optim.AdamW(params, lr=cfg.base_lr, weight_decay=cfg.weight_decay)
    else:
        raise ValueError("Unknown optimizer")


def build_scheduler(cfg: Config, optimizer):
    if cfg.scheduler == "step":
        return StepLR(optimizer, step_size=cfg.step_size, gamma=cfg.step_gamma)
    elif cfg.scheduler == "cosine":
        return CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    elif cfg.scheduler == "onecycle":
        return OneCycleLR(optimizer, max_lr=cfg.max_lr, steps_per_epoch=1, epochs=cfg.epochs)
    elif cfg.scheduler == "none":
        return None
    else:
        raise ValueError("Unknown scheduler")# Models

def build_model(model_name: str, num_classes: int, pretrained: bool = True, freeze_backbone: bool = False) -> nn.Module:
    model_name = model_name.lower()

    if model_name == "resnet18":
        weights = ResNet18_Weights.DEFAULT if pretrained else None
        model = models.resnet18(weights=weights)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)
        if freeze_backbone:
            for name, param in model.named_parameters():
                if not name.startswith("fc."):
                    param.requires_grad = False

    elif model_name == "resnet34":
        weights = ResNet34_Weights.DEFAULT if pretrained else None
        model = models.resnet34(weights=weights)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)
        if freeze_backbone:
            for name, param in model.named_parameters():
                if not name.startswith("fc."):
                    param.requires_grad = False

    elif model_name == "vgg19":
        weights = VGG19_Weights.DEFAULT if pretrained else None
        model = models.vgg19(weights=weights)
        in_features = model.classifier[-1].in_features
        new_classifier = list(model.classifier.children())
        new_classifier[-1] = nn.Linear(in_features, num_classes)
        model.classifier = nn.Sequential(*new_classifier)
        if freeze_backbone:
            for name, param in model.named_parameters():
                if not name.startswith("classifier."):
                    param.requires_grad = False
    else:
        raise ValueError(f"Unknown model name: {model_name}")

    return model# Data: transforms and loaders
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]


def build_transforms(cfg: Config):
    train_tfms = transforms.Compose([
        transforms.RandomResizedCrop(cfg.image_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])
    eval_tfms = transforms.Compose([
        transforms.Resize(int(cfg.image_size * 1.14)),
        transforms.CenterCrop(cfg.image_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])
    return {"train": train_tfms, "val": eval_tfms, "test": eval_tfms}


def _seed_worker(worker_id: int):
    worker_seed = SEED + worker_id
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def create_datasets_and_loaders(cfg: Config, split_dirs: Dict[str, str]):
    tfms = build_transforms(cfg)
    image_datasets = {
        "train": datasets.ImageFolder(split_dirs["train"], transform=tfms["train"]),
        "val": datasets.ImageFolder(split_dirs["val"], transform=tfms["val"]),
        "test": datasets.ImageFolder(split_dirs["test"], transform=tfms["test"]),
    }

    class_names = image_datasets["train"].classes
    assert image_datasets["val"].classes == class_names, "val classes mismatch train classes"
    assert image_datasets["test"].classes == class_names, "test classes mismatch train classes"

    dataset_sizes = {k: len(v) for k, v in image_datasets.items()}

    generator = torch.Generator()
    generator.manual_seed(SEED)

    use_cuda = torch.cuda.is_available()
    dataloaders = {
        "train": DataLoader(
            image_datasets["train"], batch_size=cfg.batch_size, shuffle=True,
            num_workers=cfg.num_workers, pin_memory=use_cuda, persistent_workers=(cfg.num_workers > 0),
            worker_init_fn=_seed_worker, generator=generator
        ),
        "val": DataLoader(
            image_datasets["val"], batch_size=cfg.batch_size, shuffle=False,
            num_workers=cfg.num_workers, pin_memory=use_cuda, persistent_workers=(cfg.num_workers > 0),
            worker_init_fn=_seed_worker, generator=generator
        ),
        "test": DataLoader(
            image_datasets["test"], batch_size=cfg.batch_size, shuffle=False,
            num_workers=cfg.num_workers, pin_memory=use_cuda, persistent_workers=(cfg.num_workers > 0),
            worker_init_fn=_seed_worker, generator=generator
        ),
    }

    print("Classes (10):", class_names)
    print("Dataset sizes:", dataset_sizes)
    return image_datasets, dataloaders, class_names, dataset_sizes# Config
@dataclass
class Config:
    dataset_root: str = "/content/drive/MyDrive/imagenet_subsets"
    dataset_id: str = "dataset1"
    image_size: int = 224
    batch_size: int = 64
    num_workers: int = 2
    epochs: int = 15
    base_lr: float = 1e-3
    weight_decay: float = 1e-4
    optimizer: str = "adamw"  # sgd | adam | adamw
    scheduler: str = "cosine"  # step | cosine | onecycle | none
    step_size: int = 7
    step_gamma: float = 0.1
    max_lr: float = 3e-3
    save_dir: str = "/content"

cfg = Config()


def resolve_split_dirs(cfg: Config) -> Dict[str, str]:
    root = os.path.join(cfg.dataset_root, cfg.dataset_id)
    return {
        "train": os.path.join(root, "train"),
        "val": os.path.join(root, "val"),
        "test": os.path.join(root, "test"),
    }

split_dirs = resolve_split_dirs(cfg)
print("Using dataset dirs:", split_dirs)
print("Device:", get_device())# Setup
import os
import time
import copy
import random
from dataclasses import dataclass
from typing import Dict, Tuple, List

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, OneCycleLR
from torch.utils.data import DataLoader

from torchvision import transforms, datasets, models
from torchvision.models import ResNet18_Weights, ResNet34_Weights, VGG19_Weights

import numpy as np
import matplotlib.pyplot as plt

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


def get_device():
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def format_time(seconds: float) -> str:
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h)}h {int(m)}m {s:0.1f}s"### Assignment 2: Image Classification with Pretrained CNNs

Colab-ready notebook to fine-tune pretrained CNNs (ResNet-18, ResNet-34, VGG-19) on a 10-class ImageNet subset.

- Proper transforms, normalization, and augmentations
- Replace classifier heads for 10 classes
- Training/validation with schedulers and checkpointing
- Test evaluation and accuracy reporting
- Basic plots and hyperparameter notes

Setup:
- If using Google Drive, mount it and set paths accordingly.
- Set `dataset_root` and `dataset_id` below.
- Enable GPU (Runtime → Change runtime type → GPU).